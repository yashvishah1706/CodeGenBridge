{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69a93ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation/evaluator.py\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import tempfile\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "class CodeEvaluator:\n",
    "    def __init__(self, custom_model, copilot_model, output_dir=\"../results\"):\n",
    "        \"\"\"Initialize the code evaluator\"\"\"\n",
    "        self.custom_model = custom_model\n",
    "        self.copilot_model = copilot_model\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize rouge scorer for text similarity\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    def functional_correctness(self, code, test_code):\n",
    "        \"\"\"Evaluate if the code passes the test cases\"\"\"\n",
    "        # Create a temporary file to run the code\n",
    "        with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as f:\n",
    "            f.write((code + '\\n\\n' + test_code).encode('utf-8'))\n",
    "            temp_file = f.name\n",
    "        \n",
    "        try:\n",
    "            # Run the test code\n",
    "            result = subprocess.run(\n",
    "                ['python', temp_file],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=5  # Limit execution time to avoid infinite loops\n",
    "            )\n",
    "            \n",
    "            # Check if tests passed\n",
    "            success = result.returncode == 0\n",
    "            return {\n",
    "                'success': success,\n",
    "                'stdout': result.stdout,\n",
    "                'stderr': result.stderr\n",
    "            }\n",
    "            \n",
    "        except subprocess.TimeoutExpired:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'stdout': '',\n",
    "                'stderr': 'Execution timed out'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'stdout': '',\n",
    "                'stderr': str(e)\n",
    "            }\n",
    "        finally:\n",
    "            # Clean up\n",
    "            if os.path.exists(temp_file):\n",
    "                os.remove(temp_file)\n",
    "    \n",
    "    def measure_code_complexity(self, code):\n",
    "        \"\"\"Measure code complexity metrics\"\"\"\n",
    "        # Simple complexity metrics\n",
    "        line_count = len(code.strip().split('\\n'))\n",
    "        char_count = len(code)\n",
    "        \n",
    "        # Count control structures as a rough proxy for cyclomatic complexity\n",
    "        control_keywords = [\n",
    "            'if ', 'else:', 'elif ', 'for ', 'while ', 'try:', 'except', \n",
    "            'with ', 'def ', 'class '\n",
    "        ]\n",
    "        \n",
    "        control_count = sum(code.count(keyword) for keyword in control_keywords)\n",
    "        \n",
    "        return {\n",
    "            'line_count': line_count,\n",
    "            'char_count': char_count,\n",
    "            'control_count': control_count\n",
    "        }\n",
    "    \n",
    "    def measure_similarity(self, generated_code, reference_code):\n",
    "        \"\"\"Measure similarity between generated code and reference solution\"\"\"\n",
    "        # Use ROUGE scores for similarity\n",
    "        scores = self.scorer.score(reference_code, generated_code)\n",
    "        \n",
    "        return {\n",
    "            'rouge1': scores['rouge1'].fmeasure,\n",
    "            'rouge2': scores['rouge2'].fmeasure,\n",
    "            'rougeL': scores['rougeL'].fmeasure\n",
    "        }\n",
    "    \n",
    "    def measure_generation_time(self, model, prompt):\n",
    "        \"\"\"Measure code generation time\"\"\"\n",
    "        start_time = time.time()\n",
    "        _ = model.generate_code(prompt)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        return end_time - start_time\n",
    "    \n",
    "    def evaluate_models(self, test_dataset_path=\"../data/human_eval_dataset.json\", num_samples=10):\n",
    "        \"\"\"Evaluate and compare both models on the test dataset\"\"\"\n",
    "        # Load test dataset\n",
    "        with open(test_dataset_path, 'r') as f:\n",
    "            test_data = json.load(f)\n",
    "        \n",
    "        # Limit samples if needed\n",
    "        test_data = test_data[:num_samples]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, item in enumerate(test_data):\n",
    "            print(f\"Evaluating sample {i+1}/{len(test_data)}\")\n",
    "            prompt = item['prompt']\n",
    "            reference_solution = item['canonical_solution']\n",
    "            test_code = item['test']\n",
    "            \n",
    "            # Generate code with both models\n",
    "            custom_code = self.custom_model.generate_code(prompt)\n",
    "            copilot_code = self.copilot_model.generate_code(prompt)\n",
    "            \n",
    "            # Measure generation time\n",
    "            custom_time = self.measure_generation_time(self.custom_model, prompt)\n",
    "            copilot_time = self.measure_generation_time(self.copilot_model, prompt)\n",
    "            \n",
    "            # Evaluate functional correctness\n",
    "            custom_correctness = self.functional_correctness(\n",
    "                custom_code, test_code\n",
    "            )\n",
    "            copilot_correctness = self.functional_correctness(\n",
    "                copilot_code, test_code\n",
    "            )\n",
    "            \n",
    "            # Measure code complexity\n",
    "            custom_complexity = self.measure_code_complexity(custom_code)\n",
    "            copilot_complexity = self.measure_code_complexity(copilot_code)\n",
    "            \n",
    "            # Measure similarity to reference solution\n",
    "            custom_similarity = self.measure_similarity(custom_code, reference_solution)\n",
    "            copilot_similarity = self.measure_similarity(copilot_code, reference_solution)\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'task_id': item['task_id'],\n",
    "                'prompt': prompt,\n",
    "                'custom_code': custom_code,\n",
    "                'copilot_code': copilot_code,\n",
    "                'reference_solution': reference_solution,\n",
    "                \n",
    "                'custom_success': custom_correctness['success'],\n",
    "                'copilot_success': copilot_correctness['success'],\n",
    "                \n",
    "                'custom_time': custom_time,\n",
    "                'copilot_time': copilot_time,\n",
    "                \n",
    "                'custom_line_count': custom_complexity['line_count'],\n",
    "                'copilot_line_count': copilot_complexity['line_count'],\n",
    "                'custom_control_count': custom_complexity['control_count'],\n",
    "                'copilot_control_count': copilot_complexity['control_count'],\n",
    "                \n",
    "                'custom_rouge1': custom_similarity['rouge1'],\n",
    "                'copilot_rouge1': copilot_similarity['rouge1'],\n",
    "                'custom_rougeL': custom_similarity['rougeL'],\n",
    "                'copilot_rougeL': copilot_similarity['rougeL'],\n",
    "            })\n",
    "        \n",
    "        # Save detailed results\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(os.path.join(self.output_dir, 'detailed_results.csv'), index=False)\n",
    "        \n",
    "        # Compute and save summary metrics\n",
    "        summary = {\n",
    "            'custom_success_rate': results_df['custom_success'].mean(),\n",
    "            'copilot_success_rate': results_df['copilot_success'].mean(),\n",
    "            \n",
    "            'custom_avg_time': results_df['custom_time'].mean(),\n",
    "            'copilot_avg_time': results_df['copilot_time'].mean(),\n",
    "            \n",
    "            'custom_avg_lines': results_df['custom_line_count'].mean(),\n",
    "            'copilot_avg_lines': results_df['copilot_line_count'].mean(),\n",
    "            \n",
    "            'custom_avg_rouge1': results_df['custom_rouge1'].mean(),\n",
    "            'copilot_avg_rouge1': results_df['copilot_rouge1'].mean(),\n",
    "            'custom_avg_rougeL': results_df['custom_rougeL'].mean(),\n",
    "            'copilot_avg_rougeL': results_df['copilot_rougeL'].mean(),\n",
    "        }\n",
    "        \n",
    "        summary_df = pd.DataFrame([summary])\n",
    "        summary_df.to_csv(os.path.join(self.output_dir, 'summary_results.csv'), index=False)\n",
    "        \n",
    "        return results_df, summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a626e79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (codegen-env)",
   "language": "python",
   "name": "codegen-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
